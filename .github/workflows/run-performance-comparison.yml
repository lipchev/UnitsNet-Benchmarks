name: UnitsNet Performance Comparison
on:
  workflow_dispatch:
    inputs:
      benchmark-name:
        description: 'A name given for this benchmark'
        required: false
        default: 'UnitsNet Benchmarks'
      benchmarks:
        description: 'The list of benchmarks to execute [Micro, Gradient]'
        required: false
        default: 'Micro'
      runtimes:
        description: 'The runtime version to use (e.g. net472 net48 netcoreapp21 netcoreapp31 netcoreapp50)'
        default: net472 netcoreapp21 netcoreapp50
        required: true
      exporters:
        description: 'The exporter(s) used for this run (GitHub/StackOverflow/RPlot/CSV/JSON/HTML/XML)'
        required: false
        default: fulljson rplot
      categories:
        description: 'An optional categories filter to apply'
        required: false
        default: ''
      execution-options:
        description: 'Any additional parameters passed to the benchmark'
        required: false
        default: --disableLogFile     
      baseline-repository:
        description: 'The (upstream) repository used as the baseline for the report'
        required: true
        default: 'lipchev/UnitsNet-Benchmarks'
      baseline-ref:
        description: 'The branch, tag or SHA to checkout for the (upstream) repository used as the baseline for the report'
        required: true
        default: ''     
      retention-days:
        description: 'Artifacts are retained for 90 days by default. You can specify a shorter retention period (>0)'
        required: true
        default: '90'
jobs:
  benchmark:
    runs-on: windows-latest
    steps:      
      # The benchmark ResultComparer is taken from dotnet/performance/src/tools (or possibly from the smaller lipchev/performance/src/tools/)
      - name: Download ResultComparer
        uses: actions/checkout@v2
        with:
          repository: dotnet/performance
          path: performance
          
      # checkout the baseline branch
      - name: Download baseline branch
        uses: actions/checkout@v2 
        with:
          path: 'UnitsNet'
          repository: ${{ github.event.inputs.baseline-repository }}
          ref: ${{ github.event.inputs.baseline-ref }}
          
      # we need all frameworks (even if only running one target at a time)
      - uses: actions/setup-dotnet@v1
        with:
          dotnet-version: '2.1.x'
            
      - uses: actions/setup-dotnet@v1
        with:
          dotnet-version: '3.1.x'
            
      - uses: actions/setup-dotnet@v1
        with:
          dotnet-version: '5.0.x'
            
      # executing the baseline benchmark for the current framework(s)
      - uses: ./UnitsNet/.github/actions/run-benchmark-scenario
        if: contains(github.event.inputs.benchmarks, 'Micro')
        with:
          scenario: run-micro-benchmarks.bat
          runtimes: ${{ github.event.inputs.runtimes }}
          exporters: ${{ github.event.inputs.exporters }}
          categories: ${{ github.event.inputs.categories }}
          execution-options: ${{ github.event.inputs.execution-options }}
          
      - uses: ./UnitsNet/.github/actions/run-benchmark-scenario
        if: contains(github.event.inputs.benchmarks, 'Gradient')
        with:
          scenario: run-gradient-benchmarks.bat
          runtimes: ${{ github.event.inputs.runtimes }}
          exporters: ${{ github.event.inputs.exporters }}
          categories: ${{ github.event.inputs.categories }}
          execution-options: ${{ github.event.inputs.execution-options }}
          
      - uses: ./UnitsNet/.github/actions/run-benchmark-scenario
        if: contains(github.event.inputs.benchmarks, 'Baseline')
        with:
          scenario: run-stable-baseline-benchmarks.bat
          runtimes: ${{ github.event.inputs.runtimes }}
          exporters: ${{ github.event.inputs.exporters }}
          categories: ${{ github.event.inputs.categories }}
          execution-options: ${{ github.event.inputs.execution-options }}
      
      # checkout the current branch
      - uses: dotnet/checkout@v2  

      - name: Benchmark Baseline
        run: ls -R      
        working-directory: UnitsNet/Artifacts/Benchmarks
        
      # executing the benchmark for the current framework(s)
      - uses: ./UnitsNet/.github/actions/run-benchmark-scenario
        with:
          scenario: run-comparison-benchmarks.bat
          runtimes: ${{ github.event.inputs.runtimes }}
          exporters: ${{ github.event.inputs.exporters }}
          categories: ${{ github.event.inputs.categories }}
          execution-options: ${{ github.event.inputs.execution-options }}
      
      # saving the current artifact (downloadable until the expiration date of this action)        
      - name: Store benchmark result
        uses: actions/upload-artifact@v2
        with:
          name: ${{ github.event.inputs.benchmark-name }} (${{ github.event.inputs.runtimes }})
          path: UnitsNet/Artifacts/Benchmarks
          if-no-files-found: error 
          retention-days: ${{ github.event.inputs.retention-days }} 


          