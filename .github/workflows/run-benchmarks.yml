name: Run UnitsNet Benchmarks
on:
  workflow_dispatch:
    inputs:
      benchmark-name:
        description: 'A name given for this benchmark'
        required: false
        default: 'UnitsNet Benchmarks'
      benchmarks:
        description: 'The list of benchmarks to execute [Micro, Gradient]'
        required: false
        default: 'Micro'
      runtimes:
        description: 'The runtime version to use (e.g. net472 net48 netcoreapp21 netcoreapp31 netcoreapp50)'
        default: net472 netcoreapp21 netcoreapp50
        required: true
      exporters:
        description: 'The exporter(s) used for this run (GitHub/StackOverflow/RPlot/CSV/JSON/HTML/XML)'
        required: false
        default: fulljson rplot
      categories:
        description: 'An optional categories filter to apply'
        required: false
        default: ''
      execution-options:
        description: 'Any additional parameters passed to the benchmark'
        required: false
        default: --disableLogFile      
      retention-days:
        description: 'Artifacts are retained for 90 days by default. You can specify a shorter retention period (>0)'
        required: true
        default: '90'
jobs:
  benchmark:
    runs-on: windows-latest
    steps:      
      # checkout the current branch
      - uses: actions/checkout@v2
      
      # we need all frameworks (even if only running one target at a time)
      - uses: actions/setup-dotnet@v1
        with:
          dotnet-version: '2.1.x'
            
      - uses: actions/setup-dotnet@v1
        with:
          dotnet-version: '3.1.x'
            
      - uses: actions/setup-dotnet@v1
        with:
          dotnet-version: '5.0.x'
            
      # executing the benchmark for the current framework(s)
      - uses: ./.github/actions/run-benchmark-scenario
        if: contains(github.event.inputs.benchmarks, 'Micro')
        with:
          scenario: run-micro-benchmarks.bat
          runtimes: ${{ github.event.inputs.runtimes }}
          exporters: ${{ github.event.inputs.exporters }}
          categories: ${{ github.event.inputs.categories }}
          execution-options: ${{ github.event.inputs.execution-options }}
          
      - uses: ./.github/actions/run-benchmark-scenario
        if: contains(github.event.inputs.benchmarks, 'Gradient')
        with:
          scenario: run-gradient-benchmarks.bat
          runtimes: ${{ github.event.inputs.runtimes }}
          exporters: ${{ github.event.inputs.exporters }}
          categories: ${{ github.event.inputs.categories }}
          execution-options: ${{ github.event.inputs.execution-options }}
          
      - uses: ./.github/actions/run-benchmark-scenario
        if: contains(github.event.inputs.benchmarks, 'Baseline')
        with:
          scenario: run-stable-baseline-benchmarks.bat
          runtimes: ${{ github.event.inputs.runtimes }}
          exporters: ${{ github.event.inputs.exporters }}
          categories: ${{ github.event.inputs.categories }}
          execution-options: ${{ github.event.inputs.execution-options }}
            
      -run: ls Artifacts/Benchmark      
      
      # saving the current artifact (downloadable until the expiration date of this action)        
      - name: Store benchmark result
        uses: actions/upload-artifact@v2
        with:
          name: ${{ github.event.inputs.benchmark-name }} (${{ github.event.inputs.runtimes }})
          path: Artifacts/Benchmark/
          if-no-files-found: error 
          retention-days: ${{ github.event.inputs.retention-days }} 
          
          