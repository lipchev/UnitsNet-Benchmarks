name: UnitsNet Benchmarks (auto)
on:  
  push:    
    branches: [master]    
    paths:
      - "UnitsNet/*"
      - "UnitsNet/CustomCode/**"
      - "UnitsNet.Benchmark/**"
      - ".github/workflows/*"
   
env: 
  EXECUTION_OPTIONS: --disableLogFile --minIterationCount 5 --maxIterationCount 10 --warmupCount 1 # see https://benchmarkdotnet.org/articles/guides/console-args.html
  BENCHMARK_PAGES_BRANCH: gh-pages
  BENCHMARK_DATA_FOLDER: benchmarks
  STABLE_BASELINES: "BaselineBenchmarks" # TODO could use something like *Perf_Double* or System.MathBenchmarks.Double* from dotnet/performance
jobs:  

  scenarios: # loads the strategy inputs (i.e. the benchmark-scenarios.json)
    runs-on: ubuntu-latest
    outputs:
      json: ${{ steps.load-matrix.outputs.json }}
    steps:
      - uses: actions/checkout@v2
      - id: load-matrix
        run: |
          SCENARIOS=$(echo $(cat .github/workflows/benchmark-scenarios.json))
          echo "::set-output name=json::$SCENARIOS"
                              
  test: # runs all scenarios in parallel, creating the corresponding artifacts
    needs: [ scenarios ]
    #if: github.repository_owner == 'angularsen' # (by default) the workflow doesn't need to run in a fork
    runs-on: windows-latest # required by the older frameworks
    outputs: # the benchmark reports (compress json)
      results: ${{ steps.prepare-results.outputs.json }} 
    strategy: 
      matrix: 
        scenario: ${{ fromJson(needs.scenarios.outputs.json) }}
    steps:
      - run: echo "Starting benchmarks for ${{ matrix.scenario.benchmark }} (tracked = ${{ matrix.scenario.tracked }}) running on ${{ matrix.scenario.runtime }}"
      
      - run: echo "Benchmark options ${{ matrix.scenario.benchmark-options }}"
      
      - id: cpuinfo
        run: |
          $Processor = (Get-CimInstance CIM_Processor).Name
          echo ('Processor Name: ' + $Processor)
          echo "::set-output name=processor::$Processor"
      
      # Inspired by: https://github.com/dotnet/performance/blob/3852b31dc84afef3c6b4fa52c1e440af5627b94a/scripts/benchmarks_ci.py#L200
      # net472/net48 appear to be working fine on my machine even without the work-around but fail without it on the CI (same with the nightly BDN build)
      - name: Configure target framework
        id: target-frameworks
        run: |
          $Frameworks = if ('${{ matrix.scenario.runtime}}'.StartsWith('net4')){
            '${{ matrix.scenario.framework }};${{ matrix.scenario.runtime}}'
          } 
          else {
            '${{ matrix.scenario.framework }}'
          }
          echo "::set-output name=frameworks::$Frameworks"
        shell: pwsh           
        
      - uses: actions/checkout@v2.3.1
      
      - name: Benchmark scenario execution (Micro)
        env:
          PERFLAB_TARGET_FRAMEWORKS: ${{ steps.target-frameworks.outputs.frameworks }}
        if: contains(matrix.scenario.Benchmark, 'Micro')
        run: .\UnitsNet.Benchmark\Scripts\run-micro-benchmarks.bat ${{ matrix.scenario.runtime }} -f ${{ matrix.scenario.framework }} ${{ matrix.scenario.benchmark-options }} ${{ env.EXECUTION_OPTIONS }}
      
      - name: Results
        id: prepare-results
        run: |
          $Results = Get-ChildItem -Path Artifacts\Benchmark -Filter *.json -Recurse | 
            ForEach-Object { 
              $d=(Get-Date $_.LastWriteTime.DateTime -UFormat %s); 
              (Get-Content -Raw $_.FullName | ConvertFrom-Json | 
              select -Property HostEnvironmentInfo, 
              @{Name = 'Framework'; Expression = {'netcoreapp50'}}, 
              @{Name = 'Timestamp'; Expression = {$d}}, 
              @{Name = 'Benchmarks'; Expression = {$_.Benchmarks | select -Property * -ExcludeProperty Metrics, Measurements}} )} | 
            ConvertTo-Json -Depth 20 -Compress
          echo "::set-output name=json::$Results"
        shell: pwsh           
   
  test-publish-results: # commits all artifacts for a given scenario to the benchmark branch(max-parallel: 1) 
    needs: [test]
    runs-on: ubuntu-latest
    strategy:
      max-parallel: 1 # cannot commit on the same branch in parallel
      matrix: 
        scenario: ${{ fromJson(needs.test.outputs.json) }}
    steps:                 
      # The benchmarks reporistory (e.g. gh-pages) is checked out 
      - name: Benchmark reporistory checkout ️
        uses: actions/checkout@v2
        with:
          ref: ${{ env.BENCHMARK_PAGES_BRANCH }}
          
      - name: Input report
        run: echo ${{ steps.test.outputs.json }}
        
          
  benchmark: # runs all scenarios in parallel, creating the corresponding artifacts
    needs: [ scenarios ]
    if: github.repository_owner == 'angularsen' # (by default) the workflow doesn't need to run in a fork
    runs-on: windows-latest # required by the older frameworks
    strategy: 
      matrix: 
        scenario: ${{ fromJson(needs.scenarios.outputs.json) }}
    steps:
      - run: echo "Starting benchmarks for ${{ matrix.scenario.benchmark }} (tracked = ${{ matrix.scenario.tracked }}) running on ${{ matrix.scenario.runtime }}"
      
      - run: echo "Benchmark options ${{ matrix.scenario.benchmark-options }}"
      
      - name: "Baseline comparison ${{ matrix.scenario.scaled }}"
        if: matrix.scenario.scaled
        run: echo "Baseline options ${{ matrix.scenario.baseline-options }}"  
              
      # checkout the current branch
      - uses: actions/checkout@v2.3.1
        with:
          path: UnitsNet
           
      # checkout the ResultRescaler
      - name: Checkout the ResultRescaler
        if: matrix.scenario.scaled
        uses: actions/checkout@v2
        with:
          repository: lipchev/performance
          path: performance
          
      # checkout the baseline branch
      - name: Checkout previous results 
        if: matrix.scenario.scaled
        uses: actions/checkout@v2
        with:
          ref: ${{ env.BENCHMARK_PAGES_BRANCH }}
          path: gh-pages
       
      # Inspired by: https://github.com/dotnet/performance/blob/3852b31dc84afef3c6b4fa52c1e440af5627b94a/scripts/benchmarks_ci.py#L200
      # net472/net48 appear to be working fine on my machine even without the work-around but fail without it on the CI (same with the nightly BDN build)
      - name: Configure target framework
        id: target-frameworks
        run: |
          $Frameworks = if ('${{ matrix.scenario.runtime}}'.StartsWith('net4')){
            '${{ matrix.scenario.framework }};${{ matrix.scenario.runtime}}'
          } 
          else {
            '${{ matrix.scenario.framework }}'
          }
          echo "::set-output name=frameworks::$Frameworks"
        shell: pwsh           
      
      # executing the benchmark for the current framework(s)
      - name: Benchmark scenario execution (Micro)
        env:
          PERFLAB_TARGET_FRAMEWORKS: ${{ steps.target-frameworks.outputs.frameworks }}
        if: contains(matrix.scenario.Benchmark, 'Micro')
        run: .\UnitsNet\UnitsNet.Benchmark\Scripts\run-micro-benchmarks.bat ${{ matrix.scenario.runtime }} -f ${{ matrix.scenario.framework }} ${{ matrix.scenario.benchmark-options }} ${{ env.EXECUTION_OPTIONS }}
          
      - name: Benchmark execution (Gradient) 
        env:
          PERFLAB_TARGET_FRAMEWORKS: ${{ steps.target-frameworks.outputs.frameworks }}
        if: contains(matrix.scenario.Benchmark, 'Gradient')
        run: .\UnitsNet\UnitsNet.Benchmark\Scripts\run-gradient-benchmarks.bat ${{ matrix.scenario.runtime }} -f ${{ matrix.scenario.framework }} ${{ matrix.scenario.benchmark-options }} ${{ env.EXECUTION_OPTIONS }}
                
      - name: Benchmark scenario execution (Baseline)
        env:
          PERFLAB_TARGET_FRAMEWORKS: ${{ steps.target-frameworks.outputs.frameworks }}
        if: matrix.scenario.scaled
        run: .\UnitsNet\UnitsNet.Benchmark\Scripts\run-stable-baseline-benchmarks.bat ${{ matrix.scenario.runtime }} -f ${{ matrix.scenario.framework }} ${{ matrix.scenario.baseline-options }} ${{ env.EXECUTION_OPTIONS }}
      
      # '*-report-rescaled.json' is generated from gh-pages\Baseline\*\results\'*-chart-scale.json' and UnitsNet\Artifacts\Benchmark\*\results\'*-report-full.json'
      - name: Stable baseline comparison
        if: matrix.scenario.scaled
        env:
          PERFLAB_TARGET_FRAMEWORKS: ${{ matrix.scenario.framework }}
        run: |
          if (Test-Path .\gh-pages\${{ env.BENCHMARK_DATA_FOLDER }}\${{ matrix.scenario.runtime }}\Baseline ) {
            echo "Running the ResultRescaler using the original baseline"
            Get-ChildItem -Path .\gh-pages\${{ env.BENCHMARK_DATA_FOLDER }}\${{ matrix.scenario.runtime }}\Baseline *-report-scale.json -Recurse |
            % {Copy-Item -Force -Path $_.FullName -Destination $_.FullName.Replace("-chart-scale.json", "-report-full.json") }
            dotnet run --project 'performance/src/tools/ResultRescaler' -c Release -f ${{ matrix.scenario.framework }} `
            --base gh-pages\${{ env.BENCHMARK_DATA_FOLDER }}\${{ matrix.scenario.runtime }}\Baseline `
            --diff UnitsNet\Artifacts\Benchmark `
            --stable ${{ env.STABLE_BASELINES }}       
          }else{ 
            echo "No previous baseline found (using current results)"
            Get-ChildItem -Path .\UnitsNet\Artifacts\Benchmark\ *-report-full.json -Recurse | 
            % {Copy-Item -Path $_.FullName -Destination $_.FullName.Replace("-report-full.json", "-report-rescaled.json") }
            Get-ChildItem -Path .\UnitsNet\Artifacts\Benchmark\Baseline *-report-full.json -Recurse | 
            % {Copy-Item -Path $_.FullName -Destination $_.FullName.Replace("-report-full.json", "-chart-scale.json") }
          }      
                
      # saving the current artifact (downloadable until the expiration date of this action)        
      - name: Store micro benchmark artifact
        if: contains(matrix.scenario.Benchmark, 'Micro')
        uses: actions/upload-artifact@v2
        with:
          name: Micro Benchmarks (${{ matrix.scenario.runtime }})
          path: UnitsNet\Artifacts\Benchmark\Micro
          
      - name: Store gradient benchmark artifact
        if: contains(matrix.scenario.Benchmark, 'Gradient')
        uses: actions/upload-artifact@v2
        with:
          name: Gradient Benchmarks (${{ matrix.scenario.runtime }})
          path: UnitsNet\Artifacts\Benchmark\Gradient
          
      - name: Store baseline benchmark artifact
        if: matrix.scenario.scaled
        uses: actions/upload-artifact@v2
        with:
          name: Baseline Benchmarks (${{ matrix.scenario.runtime }})
          path: UnitsNet\Artifacts\Benchmark\Baseline
          
  publish-results: # commits all artifacts for a given scenario to the benchmark branch, returning the list of created/updated reports (max-parallel: 1) 
    needs: [benchmark, scenarios]
    runs-on: ubuntu-latest
    outputs: # the list of reports that were created/modified
      reports: ${{ steps.update-reports.outputs.json }} 
    strategy:
      max-parallel: 1 # cannot commit on the same branch in parallel
      matrix: 
        scenario: ${{ fromJson(needs.scenarios.outputs.json) }}
    steps:                 
      # The benchmarks reporistory (e.g. gh-pages) is checked out 
      - name: Benchmark reporistory checkout ️
        uses: actions/checkout@v2
        with:
          ref: ${{ env.BENCHMARK_PAGES_BRANCH }}
      
      # The benchmark results are downloaded into the corresponding 'scenario' folder.
      - name: Download micro benchmark artifact
        if: contains(matrix.scenario.Benchmark, 'Micro')
        uses: actions/download-artifact@v2
        with:
          name: Micro Benchmarks (${{ matrix.scenario.runtime }})
          path: ${{ env.BENCHMARK_DATA_FOLDER }}/${{ matrix.scenario.runtime }}/Micro        
          
      - name: Download gradient benchmark artifact
        if: contains(matrix.scenario.Benchmark, 'Gradient')
        uses: actions/download-artifact@v2
        with:
          name: Gradient Benchmarks (${{ matrix.scenario.runtime }})
          path: ${{ env.BENCHMARK_DATA_FOLDER }}/${{ matrix.scenario.runtime }}/Gradient  
          
      - name: Download baseline benchmark artifact
        if: matrix.scenario.scaled
        uses: actions/download-artifact@v2
        with:
          name: Baseline Benchmarks (${{ matrix.scenario.runtime }})
          path: ${{ env.BENCHMARK_DATA_FOLDER }}/${{ matrix.scenario.runtime }}/Baseline  
          
      # Copy all of the results to the target framework folder and output the (json) list of updated json files
      - name: Update benchmark reports
        id: update-reports
        if: matrix.scenario.tracked
        run: |
          $Reports = (git status -s *report*.json).Substring(3) | ConvertTo-Json -Compress
          echo "::set-output name=json::$Reports"
        shell: pwsh
      
      # Push the modifications to the benchmarks branch
      - name: Git Auto Commit
        uses: stefanzweifel/git-auto-commit-action@v4.11.0
        with:
          commit_message: Automatic benchmark generation for ${{ github.sha }} (${{ matrix.scenario.runtime }})  
          
      - name: Result
        run: echo ${{ steps.update-reports.outputs.json }}
  
  get-reports: # get all reports currently present on the benchmark branch (obsolete)
    if: false # break glass in case of emergency
    runs-on: ubuntu-latest
    outputs:
      reports: ${{ steps.path.outputs.json }}        
    steps:
      - name: Initializing git folder ️
        uses: actions/checkout@v2.3.1
        with:
          ref: ${{ env.BENCHMARK_PAGES_BRANCH }}
          
      - name: Result path configuration
        id: path
        run: |
          $Reports = (Get-ChildItem -Filter *report*.json -Recurse | Resolve-Path -Relative).replace('./', '') | ConvertTo-Json -Compress
          echo "::set-output name=json::$Reports"
        shell: pwsh 
        
      - name: Result
        run: echo ${{ steps.path.outputs.json }}
        
  update-history: # create/append the historic data plots for each report (added/modified)
    needs: [ get-reports ]
    #needs: [publish-results]
    runs-on: ubuntu-latest
    continue-on-error: false
    strategy:
      max-parallel: 1 # we cannot commit on the same branch in parallel (multiple points added on retry- can we 'force' this?)
      matrix: 
        report-path: ${{ fromJson(needs.get-reports.outputs.reports) }}
        #report-path: ${{ fromJson(needs.publish-results.outputs.reports) }}
    steps:        
      - name: Initializing git folder ️
        uses: actions/checkout@v2.3.1
        
      # The latest results are downloaded from the gh-pages url into the 'Artifacts/Benchmark' folder.  
      - name: Fetch report
        id: download
        uses: carlosperate/download-file-action@v1.0.3
        with:
          file-url: ${{ github.actor }}.github.io/${{ github.event.repository.name }}/${{ matrix.report-path }}
          location: Artifacts/Benchmark
              
      - name: History path selection
        id: history
        run: |
          $Name = (Get-Item ${{ steps.download.outputs.file-path }}).BaseName
          $Path = if ($Name.Contains('rescaled')) {
            (Split-Path ${{ matrix.report-path }} | Split-Path) + '/history/report-rescaled'
          }
          else{
            (Split-Path ${{ matrix.report-path }} | Split-Path) + '/history/report-full'
          }
          echo "::set-output name=target-name::$Name"
          echo "::set-output name=target-path::$Path"
        shell: pwsh
          
      # appending to the running benchmark data on the benchmark-pages branch
      - name: Updating benchmark charts
        if: contains(steps.history.outputs.target-path, 'history')  
        uses: starburst997/github-action-benchmark@v1.8.7
        with:
          name: ${{ steps.history.outputs.target-name }}
          tool: 'benchmarkdotnet'
          output-file-path: ${{ steps.download.outputs.file-path }}
          gh-pages-branch: ${{ env.BENCHMARK_PAGES_BRANCH }}
          benchmark-data-dir-path: ${{ steps.history.outputs.target-path }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          # Show alert with commit comment on detecting possible performance regression
          alert-threshold: '200%'
          comment-always: false
          comment-on-alert: ${{ contains(steps.history.outputs.target-path, 'rescaled') }}
          fail-on-alert: false
          alert-comment-cc-users: '@lipchev'
        
      - name: Result
        run: echo http://${{ github.actor }}.github.io/${{ github.event.repository.name }}/${{ steps.history.outputs.target-path }}/
