name: UnitsNet Benchmarks (auto)
on:  
  push:    
    branches: [master]    
    paths:
      - "UnitsNet/*"
      - "UnitsNet/CustomCode/**"
      - "UnitsNet.Benchmark/**"
      - ".github/workflows/*"
   
env: 
  EXECUTION_OPTIONS: --disableLogFile --job Dry --allCategories Construction Unit Value # see https://benchmarkdotnet.org/articles/guides/console-args.html
  BENCHMARK_PAGES_BRANCH: gh-pages
  BENCHMARK_DATA_FOLDER: benchmarks
  BENCHMARK_DATA_FILE: benchmark-data.json
  STABLE_BASELINES: "BaselineBenchmarks" # TODO could use something like *Perf_Double* or System.MathBenchmarks.Double* from dotnet/performance
jobs:  

  scenarios: # loads the strategy inputs (i.e. the benchmark-scenarios.json)
    runs-on: ubuntu-latest
    outputs:
      json: ${{ steps.load-matrix.outputs.json }}
    steps:
      - uses: actions/checkout@v2
      - id: load-matrix
        run: |
          SCENARIOS=$(echo $(cat .github/workflows/benchmark-scenarios.json))
          echo "::set-output name=json::$SCENARIOS"
                              
  benchmarks: # runs all scenarios in parallel, creating the corresponding artifacts
    needs: [ scenarios ]
    #if: github.repository_owner == 'angularsen' # (by default) the workflow doesn't need to run in a fork
    runs-on: windows-latest # required by the older frameworks
    strategy: 
      matrix: 
        scenario: ${{ fromJson(needs.scenarios.outputs.json) }}
    steps:
      - run: echo "Starting benchmarks for ${{ matrix.scenario.benchmark }} (tracked = ${{ matrix.scenario.tracked }}) running on ${{ join(matrix.scenario.runtimes, ', ') }}"
      
      - run: echo "Benchmark options ${{ matrix.scenario.executionOptions }}"
      
      - id: cpuinfo
        run: |
          $Processor = (Get-CimInstance CIM_Processor).Name
          echo ('Processor Name: ' + $Processor)
          echo "::set-output name=processor::$Processor"
      
      # Inspired by: https://github.com/dotnet/performance/blob/3852b31dc84afef3c6b4fa52c1e440af5627b94a/scripts/benchmarks_ci.py#L200
      # net472/net48 appear to be working fine on my machine even without the work-around but fail without it on the CI (same with the nightly BDN build)
      - name: Configure target framework
        id: target-frameworks
        run: |
          $Frameworks = if ('${{ matrix.scenario.runtimes[0]}}'.StartsWith('net4')){
            '${{ matrix.scenario.framework }};${{ join(matrix.scenario.runtimes, ';') }}'
          } 
          else {
            '${{ matrix.scenario.framework }}'
          }
          echo "::set-output name=frameworks::$Frameworks"
        shell: pwsh           
        
      - uses: actions/checkout@v2.3.1
      
      - name: Benchmark scenario execution (Micro)
        env:
          PERFLAB_TARGET_FRAMEWORKS: ${{ steps.target-frameworks.outputs.frameworks }}
        if: contains(matrix.scenario.Benchmark, 'Micro')
        run: .\UnitsNet.Benchmark\Scripts\run-micro-benchmarks.bat ${{ join(matrix.scenario.runtimes, ' ') }} -f ${{ matrix.scenario.framework }} ${{ matrix.scenario.executionOptions }} ${{ env.EXECUTION_OPTIONS }}

      - name: Benchmark execution (Gradient) 
        env:
          PERFLAB_TARGET_FRAMEWORKS: ${{ steps.target-frameworks.outputs.frameworks }}
        if: contains(matrix.scenario.Benchmark, 'Gradient')
        run: .\UnitsNet\UnitsNet.Benchmark\Scripts\run-gradient-benchmarks.bat ${{ join(matrix.scenario.runtimes, ' ') }} -f ${{ matrix.scenario.framework }} ${{ matrix.scenario.executionOptions }} ${{ env.EXECUTION_OPTIONS }}
        
      # saving the current artifact (downloadable until the expiration date of this action)        
      - name: Store micro benchmark artifact
        if: contains(matrix.scenario.Benchmark, 'Micro')
        uses: actions/upload-artifact@v2
        with:
          name: ${{ matrix.scenario.Benchmark }} (${{ join(matrix.scenario.runtimes, '-') }})
          path: Artifacts\Benchmark\Micro
          
      - name: Store gradient benchmark artifact
        if: contains(matrix.scenario.Benchmark, 'Gradient')
        uses: actions/upload-artifact@v2
        with:
          name: ${{ matrix.scenario.Benchmark }} (${{ join(matrix.scenario.runtimes, '-')  }})
          path: Artifacts\Benchmark\Gradient
        
   
  publish-results: # commits all artifacts for a given scenario to the benchmark branch(max-parallel: 1) 
    needs: [scenarios, benchmarks]
    runs-on: ubuntu-latest
    steps:                 
      # The benchmarks repository (e.g. gh-pages) is checked out 
      - name: Benchmark repository checkout ️
        uses: actions/checkout@v2
        with:
          ref: ${{ env.BENCHMARK_PAGES_BRANCH }}
        
     # The benchmark results are downloaded into a corresponding 'artifact' folder.
      - name: Download benchmark artifacts
        uses: actions/download-artifact@v2
        with:
          path: Artifacts

      
      - name: Global env
        run: echo "TEST-VAR=$(echo $GITHUB_SHA | cut -c 1-6)" >> $env:GITHUB_ENV 
 
      - name: Update reports data
        run: |          
          $scenarios = ConvertFrom-Json '${{ needs.scenarios.outputs.json }}'
          foreach ($scenario in $scenarios) {
            $runtimesFolder = $scenario.runtimes -join '-'
            $artifact = Join-Path $scenario.Benchmark $runtimesFolder
            $targetDir = Join-Path ${{ env.BENCHMARK_DATA_FOLDER }} $runtimesFolder
            
            if(!(Test-Path -Path $targetDir )){
                New-Item -ItemType directory -Path $targetDir
            }

            Copy-Item $artifact $targetDir -Recurse -Force -Include $scenario.filesIncluded -Exclude $scenario.filesExcluded -verbose
            
            if ($scenario.tracked){ continue }
            
            $results += Get-ChildItem -Path $artifact -Filter *.json -Recurse | 
                ForEach-Object { 
                  (Get-Content -Raw $_.FullName | ConvertFrom-Json | 
                  select -Property HostEnvironmentInfo, 
                  @{Name = 'Category'; Expression = {$_.Title.Substring(0, $_.Title.Length - 16)}}, 
                  @{Name = 'Timestamp'; Expression = {[DateTimeOffset]::ParseExact($_.Title.Substring($_.Title.Length - 15), 'yyyyMMdd-HHmmss', [System.Globalization.CultureInfo]::InvariantCulture).ToUnixTimeSeconds()}}, 
                  @{Name = 'Benchmarks'; Expression = {$_.Benchmarks | 
                    select -Property *, 
                    @{Name = 'Runtime'; Expression = {$_.DisplayInfo.Substring($_.DisplayInfo.IndexOf('Runtime=') + 8).Split(',)')[0]}},
                    @{Name = 'Toolchain'; Expression = {$_.DisplayInfo.Substring($_.DisplayInfo.IndexOf('Toolchain=') + 10).Split(',)')[0]}} -ExcludeProperty Metrics, Measurements}})
                }             
          }          
          
          Remove-Item Artifacts -Recurse          
          if ($results.Count -eq 0) { exit }
          
          // creating / appending to the benchmark history data file
          $dataFile = Join-Path ${{ env.BENCHMARK_DATA_FOLDER }} ${{ env.BENCHMARK_DATA_FILE }}
          $headCommit = ${{ github.event.head_commit }} | ConvertFrom-Json 
          $commitBenchmarks = @{Commit = $headCommit; Benchmarks = $results }
          if (Test-Path $dataFile ){
            $historicData = $commitBenchmarks + (Get-Content $dataFile | ConvertFrom-Json )
          }
          else{
            $historicData = @($commitBenchmarks)           
          }
          $historicData | ConvertTo-Json -Depth 20 | Out-File $dataFile
        shell: pwsh
        
      - name: Benchmark reports summary
        run: |
          git status
          git diff Join-Path ${{ env.BENCHMARK_DATA_FOLDER }} ${{ env.BENCHMARK_DATA_FILE }}
        shell: pwsh         
        
          
  benchmark-old: # runs all scenarios in parallel, creating the corresponding artifacts
    needs: [ scenarios ]
    if: github.repository_owner == 'angularsen' # (by default) the workflow doesn't need to run in a fork
    runs-on: windows-latest # required by the older frameworks
    strategy: 
      matrix: 
        scenario: ${{ fromJson(needs.scenarios.outputs.json) }}
    steps:
      - run: echo "Starting benchmarks for ${{ matrix.scenario.benchmark }} (tracked = ${{ matrix.scenario.tracked }}) running on ${{ matrix.scenario.runtimes }}"
      
      - run: echo "Benchmark options ${{ matrix.scenario.executionOptions }}"
      
      - name: "Baseline comparison ${{ matrix.scenario.scaled }}"
        if: matrix.scenario.scaled
        run: echo "Baseline options ${{ matrix.scenario.baseline-options }}"  
              
      # checkout the current branch
      - uses: actions/checkout@v2.3.1
        with:
          path: UnitsNet
           
      # checkout the ResultRescaler
      - name: Checkout the ResultRescaler
        if: matrix.scenario.scaled
        uses: actions/checkout@v2
        with:
          repository: lipchev/performance
          path: performance
          
      # checkout the baseline branch
      - name: Checkout previous results 
        if: matrix.scenario.scaled
        uses: actions/checkout@v2
        with:
          ref: ${{ env.BENCHMARK_PAGES_BRANCH }}
          path: gh-pages
       
      # Inspired by: https://github.com/dotnet/performance/blob/3852b31dc84afef3c6b4fa52c1e440af5627b94a/scripts/benchmarks_ci.py#L200
      # net472/net48 appear to be working fine on my machine even without the work-around but fail without it on the CI (same with the nightly BDN build)
      - name: Configure target framework
        id: target-frameworks
        run: |
          $Frameworks = if ('${{ matrix.scenario.runtimes}}'.StartsWith('net4')){
            '${{ matrix.scenario.framework }};${{ matrix.scenario.runtimes}}'
          } 
          else {
            '${{ matrix.scenario.framework }}'
          }
          echo "::set-output name=frameworks::$Frameworks"
        shell: pwsh           
      
      # executing the benchmark for the current framework(s)
      - name: Benchmark scenario execution (Micro)
        env:
          PERFLAB_TARGET_FRAMEWORKS: ${{ steps.target-frameworks.outputs.frameworks }}
        if: contains(matrix.scenario.Benchmark, 'Micro')
        run: .\UnitsNet\UnitsNet.Benchmark\Scripts\run-micro-benchmarks.bat ${{ matrix.scenario.runtimes }} -f ${{ matrix.scenario.framework }} ${{ matrix.scenario.executionOptions }} ${{ env.EXECUTION_OPTIONS }}
          
      - name: Benchmark execution (Gradient) 
        env:
          PERFLAB_TARGET_FRAMEWORKS: ${{ steps.target-frameworks.outputs.frameworks }}
        if: contains(matrix.scenario.Benchmark, 'Gradient')
        run: .\UnitsNet\UnitsNet.Benchmark\Scripts\run-gradient-benchmarks.bat ${{ matrix.scenario.runtimes }} -f ${{ matrix.scenario.framework }} ${{ matrix.scenario.executionOptions }} ${{ env.EXECUTION_OPTIONS }}
                
      - name: Benchmark scenario execution (Baseline)
        env:
          PERFLAB_TARGET_FRAMEWORKS: ${{ steps.target-frameworks.outputs.frameworks }}
        if: matrix.scenario.scaled
        run: .\UnitsNet\UnitsNet.Benchmark\Scripts\run-stable-baseline-benchmarks.bat ${{ matrix.scenario.runtimes }} -f ${{ matrix.scenario.framework }} ${{ matrix.scenario.baseline-options }} ${{ env.EXECUTION_OPTIONS }}
      
      # '*-report-rescaled.json' is generated from gh-pages\Baseline\*\results\'*-chart-scale.json' and UnitsNet\Artifacts\Benchmark\*\results\'*-report-full.json'
      - name: Stable baseline comparison
        if: matrix.scenario.scaled
        env:
          PERFLAB_TARGET_FRAMEWORKS: ${{ matrix.scenario.framework }}
        run: |
          if (Test-Path .\gh-pages\${{ env.BENCHMARK_DATA_FOLDER }}\${{ matrix.scenario.runtimes }}\Baseline ) {
            echo "Running the ResultRescaler using the original baseline"
            Get-ChildItem -Path .\gh-pages\${{ env.BENCHMARK_DATA_FOLDER }}\${{ matrix.scenario.runtimes }}\Baseline *-report-scale.json -Recurse |
            % {Copy-Item -Force -Path $_.FullName -Destination $_.FullName.Replace("-chart-scale.json", "-report-full.json") }
            dotnet run --project 'performance/src/tools/ResultRescaler' -c Release -f ${{ matrix.scenario.framework }} `
            --base gh-pages\${{ env.BENCHMARK_DATA_FOLDER }}\${{ matrix.scenario.runtimes }}\Baseline `
            --diff UnitsNet\Artifacts\Benchmark `
            --stable ${{ env.STABLE_BASELINES }}       
          }else{ 
            echo "No previous baseline found (using current results)"
            Get-ChildItem -Path .\UnitsNet\Artifacts\Benchmark\ *-report-full.json -Recurse | 
            % {Copy-Item -Path $_.FullName -Destination $_.FullName.Replace("-report-full.json", "-report-rescaled.json") }
            Get-ChildItem -Path .\UnitsNet\Artifacts\Benchmark\Baseline *-report-full.json -Recurse | 
            % {Copy-Item -Path $_.FullName -Destination $_.FullName.Replace("-report-full.json", "-chart-scale.json") }
          }      
                
      # saving the current artifact (downloadable until the expiration date of this action)        
      - name: Store micro benchmark artifact
        if: contains(matrix.scenario.Benchmark, 'Micro')
        uses: actions/upload-artifact@v2
        with:
          name: Micro Benchmarks (${{ matrix.scenario.runtimes }})
          path: UnitsNet\Artifacts\Benchmark\Micro
          
      - name: Store gradient benchmark artifact
        if: contains(matrix.scenario.Benchmark, 'Gradient')
        uses: actions/upload-artifact@v2
        with:
          name: Gradient Benchmarks (${{ matrix.scenario.runtimes }})
          path: UnitsNet\Artifacts\Benchmark\Gradient
          
      - name: Store baseline benchmark artifact
        if: matrix.scenario.scaled
        uses: actions/upload-artifact@v2
        with:
          name: Baseline Benchmarks (${{ matrix.scenario.runtimes }})
          path: UnitsNet\Artifacts\Benchmark\Baseline
          
  publish-results-old: # commits all artifacts for a given scenario to the benchmark branch, returning the list of created/updated reports (max-parallel: 1) 
    needs: [benchmark-old, scenarios]
    runs-on: ubuntu-latest
    outputs: # the list of reports that were created/modified
      reports: ${{ steps.update-reports.outputs.json }} 
    strategy:
      max-parallel: 1 # cannot commit on the same branch in parallel
      matrix: 
        scenario: ${{ fromJson(needs.scenarios.outputs.json) }}
    steps:                 
      # The benchmarks repository (e.g. gh-pages) is checked out 
      - name: Benchmark repository checkout ️
        uses: actions/checkout@v2
        with:
          ref: ${{ env.BENCHMARK_PAGES_BRANCH }}
      
      # The benchmark results are downloaded into the corresponding 'scenario' folder.
      - name: Download micro benchmark artifact
        if: contains(matrix.scenario.Benchmark, 'Micro')
        uses: actions/download-artifact@v2
        with:
          name: Micro Benchmarks (${{ matrix.scenario.runtimes }})
          path: ${{ env.BENCHMARK_DATA_FOLDER }}/${{ matrix.scenario.runtimes }}/Micro        
          
      - name: Download gradient benchmark artifact
        if: contains(matrix.scenario.Benchmark, 'Gradient')
        uses: actions/download-artifact@v2
        with:
          name: Gradient Benchmarks (${{ matrix.scenario.runtimes }})
          path: ${{ env.BENCHMARK_DATA_FOLDER }}/${{ matrix.scenario.runtimes }}/Gradient  
          
      - name: Download baseline benchmark artifact
        if: matrix.scenario.scaled
        uses: actions/download-artifact@v2
        with:
          name: Baseline Benchmarks (${{ matrix.scenario.runtimes }})
          path: ${{ env.BENCHMARK_DATA_FOLDER }}/${{ matrix.scenario.runtimes }}/Baseline  
          
      # Copy all of the results to the target framework folder and output the (json) list of updated json files
      - name: Update benchmark reports
        id: update-reports
        if: matrix.scenario.tracked
        run: |
          $Reports = (git status -s *report*.json).Substring(3) | ConvertTo-Json -Compress
          echo "::set-output name=json::$Reports"
        shell: pwsh
      
      # Push the modifications to the benchmarks branch
      - name: Git Auto Commit
        uses: stefanzweifel/git-auto-commit-action@v4.11.0
        with:
          commit_message: Automatic benchmark generation for ${{ github.sha }} (${{ matrix.scenario.runtimes }})  
          
      - name: Result
        run: echo ${{ steps.update-reports.outputs.json }}
  
  get-reports: # get all reports currently present on the benchmark branch (obsolete)
    if: 'false' # break glass in case of emergency
    runs-on: ubuntu-latest
    outputs:
      reports: ${{ steps.path.outputs.json }}        
    steps:
      - name: Initializing git folder ️
        uses: actions/checkout@v2.3.1
        with:
          ref: ${{ env.BENCHMARK_PAGES_BRANCH }}
          
      - name: Result path configuration
        id: path
        run: |
          $Reports = (Get-ChildItem -Filter *report*.json -Recurse | Resolve-Path -Relative).replace('./', '') | ConvertTo-Json -Compress
          echo "::set-output name=json::$Reports"
        shell: pwsh 
        
      - name: Result
        run: echo ${{ steps.path.outputs.json }}
        
  update-history: # create/append the historic data plots for each report (added/modified)
    needs: [ get-reports ]
    #needs: [publish-results]
    runs-on: ubuntu-latest
    continue-on-error: false
    strategy:
      max-parallel: 1 # we cannot commit on the same branch in parallel (multiple points added on retry- can we 'force' this?)
      matrix: 
        report-path: ${{ fromJson(needs.get-reports.outputs.reports) }}
        #report-path: ${{ fromJson(needs.publish-results.outputs.reports) }}
    steps:        
      - name: Initializing git folder ️
        uses: actions/checkout@v2.3.1
        
      # The latest results are downloaded from the gh-pages url into the 'Artifacts/Benchmark' folder.  
      - name: Fetch report
        id: download
        uses: carlosperate/download-file-action@v1.0.3
        with:
          file-url: ${{ github.actor }}.github.io/${{ github.event.repository.name }}/${{ matrix.report-path }}
          location: Artifacts/Benchmark
              
      - name: History path selection
        id: history
        run: |
          $Name = (Get-Item ${{ steps.download.outputs.file-path }}).BaseName
          $Path = if ($Name.Contains('rescaled')) {
            (Split-Path ${{ matrix.report-path }} | Split-Path) + '/history/report-rescaled'
          }
          else{
            (Split-Path ${{ matrix.report-path }} | Split-Path) + '/history/report-full'
          }
          echo "::set-output name=target-name::$Name"
          echo "::set-output name=target-path::$Path"
        shell: pwsh
          
      # appending to the running benchmark data on the benchmark-pages branch
      - name: Updating benchmark charts
        if: contains(steps.history.outputs.target-path, 'history')  
        uses: starburst997/github-action-benchmark@v1.8.7
        with:
          name: ${{ steps.history.outputs.target-name }}
          tool: 'benchmarkdotnet'
          output-file-path: ${{ steps.download.outputs.file-path }}
          gh-pages-branch: ${{ env.BENCHMARK_PAGES_BRANCH }}
          benchmark-data-dir-path: ${{ steps.history.outputs.target-path }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          # Show alert with commit comment on detecting possible performance regression
          alert-threshold: '200%'
          comment-always: false
          comment-on-alert: ${{ contains(steps.history.outputs.target-path, 'rescaled') }}
          fail-on-alert: false
          alert-comment-cc-users: '@lipchev'
        
      - name: Result
        run: echo http://${{ github.actor }}.github.io/${{ github.event.repository.name }}/${{ steps.history.outputs.target-path }}/
